Web-based AR Gesture Control Simulator
Keerthi Reddy Pakanati - A04323192
 Venkata Santosh Valli Sunkarapalli - A04329821
Harishma Purella - A04335620
INTRODUCTION
Augmented Reality (AR) is a technology which gives a cohesive experience to the users by combining the real world and computer-generated 3D. In the recent years there has been a rapid growth in AR in industries like gaming, healthcare, education etc. Augmented Reality can be the future as it makes or helps the users to experience to visualize the things that are in real world which makes more interactive and enjoyable. We need HCI research in Augmented Reality for developing because it helps to design or implement the frameworks keeping the user’s view, preferences and their behavior in mind which helps to develop an effective and interactive AR system. The major drawback of developing an AR system or testing an AR application is it needs a specialized hardware which is very expensive. So, developing a Web based AR would be considered as a better alternative because of the accessibility that it allows the users to experience the AR using their internet or Web browsers without any use of software or hardware installations on their system. When compared to developing an AR using Web Browser rather than using a specific hardware it decreases the cost of implementing as it helps the developers with a low-cost approach. The other advantage can be considered with this approach is if there are any updates in the AR it can be done instantly. This development of using Web services also helps the educational and marketing backgrounds. In conclusion Web-based AR development is useful for both user and developer as it supports low-cost development to the developer and make easy for accessing to the users.
Our project Web-based AR Gesture Control Simulator aims to build a bridge for the gap between the AR and the users by developing a user-friendly platform by using an integrated Web Camera which captures the users hand gestures which helps in developing an AR environment to experiment with gesture-based features. The significance of our project is to help the designers or developers to implement a environment without any need of special hardware in the system

PROBLEM STATEMENT 
As it was previously mentioned that most of the AR applications are implemented using a specialized hardware for example the AR glasses. This web-based approach makes challenging for the developers as the user needs to try with the gesture-based controls. The problem arises for the development of web-based gesture controls as they need AR headset or motion controllers. 
Our project aims to develop a user-friendly solution where it uses the web cam which is integrated in the system to capture the gestures or the user. This approach of using the system’s webcam helps the developers in not investing in any AR equipment which is costly.
The research questions that the project aims to focus on are some of the following mentioned:
•	How accurate can a web cam capture the gestures using a web browser?
•	How can we help the users to run this application or environment in any common desktop or laptop?
•	How to create a 3D environment in web browser where it captures the hand gestures in a simple way?
•	What would be the most important feature or technique can be implemented in this web-based approach?

PROJECT OBJECTIVES
The project aims for Specific, Measurable, Achievable, Relevant and Time-bound (SMART) objectives:
•	Developing Hand Tracking Module: We are using the tensorFlow.js in the first phase of the project. This module developments helps in detecting and capturing the hand moments in real time using the web cam.
•	Creating a Virtual AR Environment: We are going to develop 2 3D rendering environment using the three.js or WebGL in second phase of the project. This implementation of the environment helps the users where they can interact with virtual objects with their hand gestures.
•	Creating a Gesture Recognition System: We will develop a gesture recognition module where we will set some predefined gestures like rotating the hand, pinching etc and mentioning them in the AR environment.
•	Designing a User-friendly Interface: We need to create a interface which provides real time experiences and feedback for the users on how it is recognizing the hand gestures in the AR environment.
•	The main objective of our project is to develop a Web-based AR Gesture Simulator Control by the end of this course.

LITERATURE REVIEW
Gesture Recognition and AR are one of those topics that are researched and have been researched from a long time but most of these research focus on using specialized hardware or custom designed hardware to successfully perform operations such as recognizing gestures. These days the apple vision pro has become the new talking point in AR research where Apple and also Meta are trying to pioneer AR research, but all of these companies focus on hardware and pushing its capabilities. 
There are very little companies that use web-based AR, one of the widely used examples of web-based AR is the Ikea app which shows the furniture in video fee, but this is still a native application not a web app. There is still a lack of user-friendly platforms that make use of web-AR and also use hand gestures.
Existing Research: 
•	Gesture based AR Interaction: Gavgiotaki et al. (2023) [1] provide a detailed understanding of how we can use gestures for AR systems in different scenarios like medicine, education etc. In this paper, they talk about categorizing gestures based on how we use them like the type of gesture and where we use it. They focus on the how user-centric is important and differentiate between “touch based” and “mid-air” gestures, talking about the tradeoffs among them. 
The other paper we referred to was, A Review of Augmented Reality-Based Human-Computer Interaction Applications of Gesture-Based Interaction by Kerdvibulvech (2019) [2] finds out different ways on how humans can interact with AR systems, he focused on hand gesture applications and diving deeper into gesture recognition and interaction and talking about how we can improve the user experience of such systems. They also talk about different scenarios where this can be used. The paper also talks about new technologies like Leap Motion and HoloLens talking about how we can improve AR-HCI through different means.
•	Hand Tracking and Gesture Recognition: In the paper, [3] Nguyen Dang Binh et al talk about a gesture recognition framework that can be used to track hand movements in real time by extracting features like hand movements, motion and hand region. They try and use different frameworks, compare them and say that a Hidden Markov Model is the best to use. The example that they have tests different corner cases like hands with and without gloves and also different backgrounds. They use their example to get a 98% accuracy in translating sign language.
We studied another paper [5] where Hui-Shyong Yeo, Byung-Gook Lee, and Hyotaek Lim talk about a low-cost system using low cast USB webcams and low cost depth cameras to track hand and finger movements in a 2D space. They used low-cost hardware to even test out complex Computer Vision tasks such as motion blur, difference between hand and face and also complex and confusing backgrounds that might throw the algorithm off. We studied this paper as we might have to deal with low-cost cameras/hardware in the client’s computer. In the paper, they used Kalman filtering and translated that into a system of inputs.
•	Web-based AR and 3D Rendering: In the paper [4]"Multi-Resolution 3D Rendering for High-Performance Web AR," Argyro-Maria Boutsi, Charalabos Ioannidis, and Styliani Verykokou talk about a prototype utilizing multi resolution 3D models to implement WebAR where they use Nexus.js for view dependent rendering and also optimizing AR applications feed and also the performance. They built the entire application using AR.js and Three.js, this prototype also supports natural feature tracking and location based AR.
Knowledge Gaps: Below are all the knowledge gaps that we’ve found researching for this project 
•	We came across a lot of research on web-based hand tracking and AR separately but there was no research that we found that combines both of these technologies together to create what we are trying to do.
•	The other gap in research that we’ve realized is that there’s a lack of HCI research in terms of AR in general where there’s no proper research on the UI/UX for an AR application, especially making web-based AR as user friendly and intuitive as possible. 
•	Due to computational limitations while using a client browser in this project, we were also thinking about the Hand Tracking Accuracy and the Rendering performance in a browser specific to simulating AR and there was limited to no research in this field as well.
METHODOLOGY
Our project is developed with the following modules, tools which are described below
•	Hand Tracking: We are going to develop the hand and finger tracking using Tensoflow.js using the MediaPipe hands model. We will integrate the tensoFlow.js and MediaPipe into the Web Application developed. Using a webcam, we are going to capture the hand moments. After capturing these moments in a video, we will process the frames into real time to detect the hand landmarks. After pointing the landmarks on hand, like joints and fingertips are used for gestures recognition.
•	Virtual AR Environment: This Augmented Reality environment is developed using three.js which is very famous and powerful JavaScript library which is used for implementing 3D graphics in a web browser. We will create a 3D modular scene graph which displays the environment. Then an optimized pipeline is needed to be created to make sure of smooth performance across all the devices used.
•	Gesture Recognition System: This system is going to use a combination of algorithms which are rule-based and Machine Learning model. We need to set some predefined gestures for the hands. We need to develop a machine learning model using the tensorFlow.js. Then create a data collection and training pipeline for the predefined gestures. For avoiding the false gestures and improving the efficiency of recognition we can implement the gesture smoothing algorithm.
•	User-Friendly Interface: For creating an interface, we are going to use the react.js using the UI/UX principles for efficiency. Develop a responsive interface based on the screen sizes and devices used. We need to develop an interactive interface for gesture mapping system for the user with AR actions. Then implement an interactive interface which gives feedback for the user gesture and their effects. Develop the interface with accessibility features for the users.
•	Other modules used for the implementation: 
	The Data management layer and the rest API are also used for the implementation of this project. We use a local storage interface for saving and loading the user preferences and their gestures. This is mainly used for implementing an efficient storage to utilize.
	The state manager is responsible for handling the complex state management using the Redux. This is used to implement the middleware for controlling the asynchronous actions
	Rest API: This is application programming interface software which is used as a communication for saving and sharing the gestures. This software is also used for accessing the date securely

Tools used for Project implementation
•	TensorFlow.js: It is a open source library used by the developers for Machine Learning and Artificial Intelligence. It is used to implement data automation, model tracking and performance monitoring. In our project we are using this tool for the gestures tracking.
•	Three.js: This tool is used for creating a virtual AR environment using the web browser. This is a famous JavaScript library which helps the developers to implement a 3D graphics and animation in the web browser.
•	HTML/CSS: HTML is a markup language used for developing the web pages and web applications. CSS know as Cascading Style Sheets is used to present interface with attractive color and beautifully.
•	MediaPipe: This platform is developed by the Google company for implementing deep learning modules across various domains which includes text, audio and computer vision.
Challenges and Limitations
•	Gesture Recognition Accuracy: Discrepancy in the lighting, background and the hand gestures may affect the accuracy of the gesture recognition, and this may be a challenge. To overcome this challenge, we can implement the options like user calibration and fine tuning for the gesture recognition.
•	Performance Constraints: Implementing a real time gesture control recognition and a 3D rendering framework in a web browser can be a challenge. To overcome this challenge, we can implement efficient rendering techniques and optimizing the model size.
•	User Interface Usability: Implementing an interface which should be easy to use for the users and dynamic is considered as the challenging part of the development. Regular user testing will be performed to make sure that the user meets the needs and expectations.
REFERENCES:
1.	Gavgiotaki, Despoina, Stavroula Ntoa, George Margetis, Konstantinos C. Apostolakis, Constantine Stephanidis, Foundation for Research and Technology - Hellas Despoina GavgiotakiInstitute of Computer Science, Foundation for Research and Technology - Hellas Stavroula NtoaInstitute of Computer Science, Foundation for Research and Technology - Hellas George MargetisInstitute of Computer Science, Foundation for Research and Technology - Hellas Konstantinos C. ApostolakisInstitute of Computer Science, and Foundation for Research and Technology - Hellas Constantine StephanidisInstitute of Computer Science. “Gesture-Based Interaction for AR Systems: A Short Review: Proceedings of the 16th International Conference on Pervasive Technologies Related to Assistive Environments.” ACM Other conferences, August 10, 2023.https://dl.acm.org/doi/10.1145/3594806.3594815#:~:text=Gesture-based%20interaction%20constitutes. 
2.	Kerdvibulvech, Chutisant. “A Review of Augmented Reality-Based Human-Computer Interaction Applications of Gesture-Based Interaction.” SpringerLink, January 1, 1970. https://link.springer.com/chapter/10.1007/978-3-030-30033-3_18#citeas. 
3.	Binh, Nguyen Dang, Enokida Shuichi and Toshiaki Ejima. “Real-Time Hand Tracking and Gesture RecognitionSystem.”(2005).
https://link.springer.com/article/10.1007/s11042-013-1501-1
4.	Boutsi, Argyro-Maria, Charalabos Ioannidis, and Styliani Verykokou. 2023. "Multi-Resolution 3D Rendering for High-Performance Web AR" Sensors 23, no. 15: 6885. https://doi.org/10.3390/s23156885
5.	Yeo, Hui-Shyong & Lee, Byung-Gook & Lim, Hyotaek. (2013). Hand tracking and gesture recognition system for human-computer interaction using low-cost hardware. Multimedia Tools and Applications. 74. 10.1007/s11042-013-1501-1.
